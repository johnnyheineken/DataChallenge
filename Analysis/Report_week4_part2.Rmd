---
title: "Team Assignment - Week 4"
author: "Stepan Svoboda, Jan Hynek, Nursultan Svankulov"
date: "2 prosince 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning = FALSE, message=FALSE, error=FALSE}
library(tidyverse)
library(keras)
library(mlr)
library(caret)

set.seed(921021)


# setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
```


# Question 10

We have altered our data in two ways since our last assignment. we will describe the way we transformed the data from our DTM and then the changes to the rest of the dataset.

We had our normalized Bag of Words (using TF-IDF) and applied on it the standard Latent Dirichlet allocation. We have decided to group our description DTM into 50 columns and our title DTM into 25 columns due to the technical constraints (our memory size). We also relied on descriptions and recommendations found throughout several documentations/implementations of LDA and some discussions related to the problem of dimensionality reduction of text data. We also considered using the Word2vec or some version of word embeddings but again due to uncertainty about the outcome and too high computational cost we have steered away and chose the functioning approach of LDA on the normalized Bag of Words.

Second part of our work with data was dimensionality reduction through changes of our factor variables, i.e. categories, regions and cities. These dummy variables had many levels and would bring a large number of additional dimensons into our dataset and we were worried that such a large number of additional dimensions would severely lower the strength of our model. We have found a way around it and created a numerical variable aimed at distinguishing between these categories/regions/cities. We took the number of promoted items and divided by the number of not promoted items in each category/city/ region and managed to squeeze into this variable both its size and share of promoted items, while significantly lowering the number of dimensions. We have also considered encoding by hand categories as well as cities and regions into more homogenous clusters, e.g. electronics, large cities, regions in the north etc. Other changes included replacing the unique user id with the number of items the specific user posted on the site. There were few other changes similar to those just described.

Later we addressed the imbalanced data set issue. We simply cannot train a model on a dataset such as this where we have only slightly more than 2% of positive observations. There are two solutions to this issue, generating more positive observations or ignoring some negative. Due to the size of our dataset and the overall imbalancedness we combined these two approaches. We have also tried the SMOTE (Synthetic minority oversampling technique) but the results were not satisfying. This is most likely due to the SMOTE picking up on too much noise and not replicating the signal in our positive observations. On top of that we were constrained by our machinery as applying algorithm such as SMOTE on dataset of this size is not feasible on our setup. Thus we have chosen to use basic under- and oversampling of our dataset to have the share of observations slightly above the 50/50 threshold, i.e. somewhere between 50 and 60% of negative observations present in dataset.

# Question 11

We start off by loading the data, under- and oversampling and preparing them for the modeling.

```{r}
gc()

load("prepared_dataset.RData")

# Needed in case for quick run:
# size_of_random_subset <- 0.01
# rnd <- runif(n = dim(d)[1])
# d <- d[rnd < size_of_random_subset, ]



rnd <- runif(n = dim(d)[1])
d_test <- d[rnd < 0.15, ] # true data for final modeling
d_train <- d[rnd >= 0.15, ]
rm(d)

factors <- c("was_promoted", 
             "visible_in_profile", "is_liquid", "excl_descr", 
             "has_capslock_descr", "excl_title", "has_capslock_title", 
             "no_price", "no_descr", "bad_cl_text", "good_cl_text",           
             "good_cl_categs",          
             "eight_to_sixteen",        
             "seventeen_to_twentyfour", 
             "m1", "m2", "m3", "m4", "m5", "m6", "m7", "m8", "m9",
             "is_weekend")


d_factors <- d_train %>% mutate_if(colnames(.) %in% factors, as.factor)
rm(d_train)
gc()


##### over/under sample
task <- makeClassifTask(data = d_factors, target = "was_promoted")
task.under <- undersample(task, rate = 1/3)
task.over <- oversample(task.under, rate = 10)

table(getTaskTargets(task.over))
d_factors <- getTaskData(task.over)

my_fun <- function(x){as.numeric(as.vector(x))}
x_train <- d_factors %>% mutate_if(colnames(.) %in% factors, my_fun)
x_test <- d_test

rm(d_test)
rm(d_factors)


y_test <- to_categorical(x_test$was_promoted)
y_train <- to_categorical(x_train$was_promoted)

x_test$was_promoted <- NULL
x_train$was_promoted <- NULL


x_train <- as.matrix(x_train)
x_test <- as.matrix(x_test)

rm(list = setdiff(ls(), c("x_test", "x_train", "y_test", "y_train")))




```

Now we train the logit model.

```{r}
logit <- keras_model_sequential() 

logit %>% 
  layer_dense(units = 2, activation = 'softmax', input_shape = c(dim(x_train)[2])) %>% 
  
summary(logit)

logit %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_adam(),
  metrics = c('accuracy')
)

history1 <- logit %>% fit(
  x_train, y_train, 
  epochs = 5, batch_size = 1024, 
  validation_split = 0.2
)
plot(history1)

pred1 <- logit %>% predict_classes(x_test)

confusionMatrix(as.numeric(pred1), y_test[, 2], positive = "1")

```


# Question 12

Here we train the neural network.

```{r}
# keras
nn <- keras_model_sequential()
nn %>% 
  layer_dense(units = 512, input_shape = c(dim(x_train)[2])) %>% 
  layer_dropout(rate = 0.4) %>% 
  layer_activity_regularization(l1 = 0.00002) %>%
  layer_activation_leaky_relu(alpha = 0.55) %>%
  
  # sofar (probably) best combinations (all in layer 1) - alpha = 0.55, l1 = 0.00003;
  # alpha = 0.6, l1 = 0.00003; alpha = 0.6, l1 = 0.00002; alpha = 0.55, l1 = 0.00001
  
  layer_dense(units = 256) %>%
  layer_dropout(rate = 0.3) %>%
  layer_activity_regularization(l1 = 0.0000145) %>%
  layer_activation_leaky_relu(alpha = 0.5) %>%
  # reg parameter can be as high as 0.00002

  layer_dense(units = 32) %>%
  layer_dropout(rate = 0.25) %>%
  layer_activation_leaky_relu(alpha = 0.3) %>%
  
  layer_dense(units = 2, activation = 'softmax')


nn %>% compile(
  loss = c('binary_crossentropy'),
  optimizer = optimizer_adam(lr = 0.00001),
  metrics = c('accuracy'))



history <- nn %>% fit(
  x_train, y_train, shuffle = T,
  epochs = 10, batch_size = 2048, 
  validation_set = 0.2)


pred <- nn %>% 
  predict_classes(x_test)


confusionMatrix(as.numeric(pred), y_test[, 2], positive = "1")

```
We get following result after 20 epochs:
```{r}
#           Reference
# Prediction      0      1
#          0 521796   9096
#          1  95656   4619
#                                          
#                Accuracy : 0.834          
#                  95% CI : (0.8331, 0.835)
#     No Information Rate : 0.9783         
#     P-Value [Acc > NIR] : 1              
#                                          
#                   Kappa : 0.0445         
#  Mcnemar's Test P-Value : <2e-16         
#                                          
#             Sensitivity : 0.336785       
#             Specificity : 0.845079       
#          Pos Pred Value : 0.046063       
#          Neg Pred Value : 0.982867       
#              Prevalence : 0.021730       
#          Detection Rate : 0.007318       
#    Detection Prevalence : 0.158872       
#       Balanced Accuracy : 0.590932       
#                                          
#        'Positive' Class : 1              
```                             


What happens if we add more epochs?
```{r}
history <- nn %>% fit(
  x_train, y_train, shuffle = T,
  epochs = 10, batch_size = 2048, 
  validation_set = 0.2)


pred <- nn %>% 
  predict_classes(x_test)


confusionMatrix(as.numeric(pred), y_test[, 2], positive = "1")
```

# Question 13

We will describe our findings first, then move on to questions and how we addressed or tried to address them.

Our models could not in simple accuracy beat the simple model of predicting only 0. This is mainly caused by our dataset being hugely imbalanced and the heuristic of classifying everyone as zero is extremely strong. We have tried many combinations and ended up with two hidden layer network with 512, 256, 32 and 2 neurons in the output layer. Using regularization and leaky relus as our activation functions led to decent results. The results were not good as we had trouble training the network and did not know how to best approach it. Our models mainly overpredicted positive observations or barely managed to detect any. In the end our best models (one of them is described here, other options are mentioned in the comments) managed to identify reasonable number of positive observations (between one fifth and half of them) while correctly classifying the negative observations as well (between two thirds and ninety percent).

Our problems and questions were plentiful. We have solved and addressed some but many are still bothering us. One of the problems is setting up the net. As we had no experience what so ever we were quite lost and stil are extremely unsure. The results fell most of the time basically random and unlike all of our previous experience with modeling (basic regression, some more complex ones, SVMs, ...)
we did not feel that after the modeling ended we had achieved as much as was with the data and the method possible in the time elapsed. We have simply no idea whether the chosen structure is good for this kind of problem (e.g. should we have used something like convolutional/recurrent net? would deeper architecture work better?). We have created a 2 hidden layer net and it felt quite ok but we simply did not manage to explore properly other possibillities and went with the option that felt as the best at the time. This could have gotten us to a place where we were getting somewhat decent results and take us astray from the path to a better model, which could have been more complex/have different structure/...

Another huge issue is that we did not know what constitutes a good result. What error rate is good? How many false positives or false negatives is it ok to have? We again have no idea and we had because of that a very hard time with tuning the model. Without knowing the real business case behind the problem it is very hard to try to optimize model. In the end we went with model that had decent accuracy with both negative and positive observations but if we knew which mistake was more costly we could have tuned our model better. Especially a tailor-made metric/loss function could have provided a lot of additional dsicriminative power.

The actions we have taken were mainly trying to find answers on google or talk with other people who were working on the same problem and see how they dealt with the same issues we had. Often the problem lied in the computational complexity and we solved that by either using different technique or lowering the complexity of task to work with smaller data (i.e. loweing dimensionality).





